#!/usr/bin/env python3
"""
Enhanced Multi-Token Analytics Tool with Data-Driven Scoring
Version: enhanced_v2.5_institutional_cascade
Features: ML-based scoring, cross-sectional ranking, correlation penalties, regime gating
"""

import asyncio
import logging
import sqlite3
import pandas as pd
import numpy as np
import json
import time
from datetime import datetime, timedelta
from dataclasses import dataclass, field, asdict
from typing import List, Dict, Optional, Tuple, Any
import os
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Enhanced imports for institutional-grade features
try:
    from sklearn.ensemble import GradientBoostingClassifier
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import classification_report
    import joblib
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    print("⚠️ ML libraries not available. Using rule-based scoring.")

# --- Configuration ---
# Attempt to import from config.py, with fallbacks if not found or incomplete
SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))

DB_PATH_DEFAULT = os.path.join(SCRIPT_DIR, 'market_data.db')
OUTPUT_FILE_DEFAULT = os.path.join(SCRIPT_DIR, 'multi_token_analysis.json')
DEBUG_FILE_DEFAULT = os.path.join(SCRIPT_DIR, 'analyzer_debug.json')
LOG_FILE_DEFAULT = os.path.join(SCRIPT_DIR, 'analyzer.log')

try:
    from config import (
        RSI_PERIOD as CFG_RSI_PERIOD,
        RSI_OVERSOLD as CFG_RSI_OVERSOLD,
        RSI_OVERBOUGHT as CFG_RSI_OVERBOUGHT,
        VOLUME_SPIKE_FACTOR as CFG_VOLUME_SPIKE_FACTOR,
        TOKEN_LIST_URL as CFG_TOKEN_LIST_URL,
        DEFAULT_TOKENS as CFG_DEFAULT_TOKENS,
        ANALYSIS_CANDLE_LIMIT as CFG_ANALYSIS_CANDLE_LIMIT,
        API_TIMEOUT as CFG_API_TIMEOUT,
        DB_PATH_OVERRIDE as CFG_DB_PATH_OVERRIDE,
        LOG_LEVEL as CFG_LOG_LEVEL,
        OUTPUT_FILE_OVERRIDE as CFG_OUTPUT_FILE_OVERRIDE,
        DEBUG_FILE_OVERRIDE as CFG_DEBUG_FILE_OVERRIDE,
        LOG_FILE_OVERRIDE as CFG_LOG_FILE_OVERRIDE,
        ML_MODEL_PATH as CFG_ML_MODEL_PATH,
        SCALER_PATH as CFG_SCALER_PATH,
        CORRELATION_WINDOW as CFG_CORRELATION_WINDOW,
        CORRELATION_PENALTY_FACTOR as CFG_CORRELATION_PENALTY_FACTOR,
        REGIME_MULTIPLIER_BEAR as CFG_REGIME_MULTIPLIER_BEAR,
        REGIME_MULTIPLIER_BULL as CFG_REGIME_MULTIPLIER_BULL,
        PERCENTILE_RANKING as CFG_PERCENTILE_RANKING,
        DATA_DRIVEN_SCORING as CFG_DATA_DRIVEN_SCORING,
        MIN_TRAINING_SAMPLES as CFG_MIN_TRAINING_SAMPLES
    )
    RSI_PERIOD = CFG_RSI_PERIOD
    RSI_OVERSOLD = CFG_RSI_OVERSOLD
    RSI_OVERBOUGHT = CFG_RSI_OVERBOUGHT
    VOLUME_SPIKE_FACTOR = CFG_VOLUME_SPIKE_FACTOR
    TOKEN_LIST_URL = CFG_TOKEN_LIST_URL
    DEFAULT_TOKENS = CFG_DEFAULT_TOKENS
    ANALYSIS_CANDLE_LIMIT = CFG_ANALYSIS_CANDLE_LIMIT
    API_TIMEOUT = CFG_API_TIMEOUT
    ANALYSIS_OUTPUT_FILE = 'multi_token_analysis.json'
    DB_PATH = CFG_DB_PATH_OVERRIDE if CFG_DB_PATH_OVERRIDE else DB_PATH_DEFAULT
    LOG_LEVEL = CFG_LOG_LEVEL
    OUTPUT_FILE = CFG_OUTPUT_FILE_OVERRIDE if CFG_OUTPUT_FILE_OVERRIDE else OUTPUT_FILE_DEFAULT
    DEBUG_FILE = CFG_DEBUG_FILE_OVERRIDE if CFG_DEBUG_FILE_OVERRIDE else DEBUG_FILE_DEFAULT
    LOG_FILE = CFG_LOG_FILE_OVERRIDE if CFG_LOG_FILE_OVERRIDE else LOG_FILE_DEFAULT
    # Ensure TOKEN_LIST_URL is a string to prevent TypeError with aiohttp
    if TOKEN_LIST_URL is None:
        TOKEN_LIST_URL = ""

    # Enhanced institutional-grade configuration
    ML_MODEL_PATH = CFG_ML_MODEL_PATH
    SCALER_PATH = CFG_SCALER_PATH
    CORRELATION_WINDOW = CFG_CORRELATION_WINDOW
    CORRELATION_PENALTY_FACTOR = CFG_CORRELATION_PENALTY_FACTOR
    REGIME_MULTIPLIER_BEAR = CFG_REGIME_MULTIPLIER_BEAR
    REGIME_MULTIPLIER_BULL = CFG_REGIME_MULTIPLIER_BULL
    PERCENTILE_RANKING = CFG_PERCENTILE_RANKING
    DATA_DRIVEN_SCORING = CFG_DATA_DRIVEN_SCORING
    MIN_TRAINING_SAMPLES = CFG_MIN_TRAINING_SAMPLES
except ImportError:
    RSI_PERIOD = 14
    RSI_OVERSOLD = 30
    RSI_OVERBOUGHT = 70
    VOLUME_SPIKE_FACTOR = 2.0
    TOKEN_LIST_URL = "https://api.binance.com/api/v3/exchangeInfo"
    DEFAULT_TOKENS = ["BTCUSDT", "ETHUSDT", "BNBUSDT", "ADAUSDT", "SOLUSDT", "XRPUSDT", "DOGEUSDT", "TRXUSDT", "LTCUSDT", "LINKUSDT", "DOTUSDT", "AVAXUSDT", "ATOMUSDT", "SHIBUSDT", "PEPEUSDT", "ARBUSDT", "OPUSDT", "NEARUSDT", "INJUSDT", "TAOUSDT", "FETUSDT", "SUIUSDT", "WIFUSDT", "TIAUSDT", "AAVEUSDT", "APTUSDT", "SUSHIUSDT", "WLDUSDT", "ETCUSDT", "TRUMPUSDT"]
    ANALYSIS_CANDLE_LIMIT = 1500
    API_TIMEOUT = 10
    ANALYSIS_OUTPUT_FILE = 'multi_token_analysis.json'
    DB_PATH = DB_PATH_DEFAULT
    LOG_LEVEL = "INFO"
    OUTPUT_FILE = OUTPUT_FILE_DEFAULT
    DEBUG_FILE = DEBUG_FILE_DEFAULT
    LOG_FILE = LOG_FILE_DEFAULT

    # Enhanced institutional-grade configuration
    ML_MODEL_PATH = 'scoring_model.pkl'
    SCALER_PATH = 'feature_scaler.pkl'
    CORRELATION_WINDOW = 90
    CORRELATION_PENALTY_FACTOR = 0.3
    REGIME_MULTIPLIER_BEAR = 0.8
    REGIME_MULTIPLIER_BULL = 0.8
    PERCENTILE_RANKING = True
    DATA_DRIVEN_SCORING = ML_AVAILABLE
    MIN_TRAINING_SAMPLES = 1000

# Threshold constants
BUY_THRESHOLD = 60  # Score threshold for buy signals

# Custom JSON encoder to handle numpy types
class CustomJSONEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, (np.bool_, np.generic)):
            return bool(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super().default(obj)

# --- Logging Setup ---
log_dir = os.path.dirname(LOG_FILE)
if log_dir and not os.path.exists(log_dir):
    os.makedirs(log_dir, exist_ok=True)

logging.basicConfig(
    level=getattr(logging, LOG_LEVEL.upper(), logging.INFO),
    format='%(asctime)s - %(levelname)s - %(module)s - %(funcName)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, mode='a'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# --- Data Structures ---
@dataclass
class TokenOpportunity:
    symbol: str
    current_price: float = 0.0
    price_change_1h: float = 0.0
    price_change_4h: float = 0.0 
    price_change_24h: float = 0.0
    rsi: float = 0.0
    volume_spike: bool = False
    support_bounce: bool = False
    resistance_rejection: bool = False
    multi_timeframe_alignment: bool = False
    composite_score: float = 0.0
    entry_recommendation: str = "NEUTRAL"
    volatility: float = 0.0
    returns_vec: List[float] = field(default_factory=list)
    # Enhanced scoring fields
    long_score: float = 0.0
    short_score: float = 0.0
    raw_long_score: float = 0.0  # Before correlation penalty and regime adjustment
    raw_short_score: float = 0.0
    percentile_long: float = 0.0  # Cross-sectional percentile rank (0-100)
    percentile_short: float = 0.0
    correlation_btc: float = 0.0  # Rolling correlation with BTC
    correlation_eth: float = 0.0  # Rolling correlation with ETH
    correlation_penalty: float = 0.0  # Applied penalty for high correlation
    regime_multiplier: float = 1.0  # Market regime adjustment factor
    direction: str = "NEUTRAL"
    # Simplified binary recommendations  
    simple_long_action: str = "DON'T_BUY_LONG"  # BUY_LONG or DON'T_BUY_LONG
    simple_short_action: str = "DON'T_SHORT"    # SHORT or DON'T_SHORT
    # Feature engineering for ML
    feature_vector: List[float] = field(default_factory=list)
    ml_confidence: float = 0.0  # Model confidence score (0-1)
    scoring_method: str = "rule_based"  # "rule_based", "ml_model", or "hybrid"
    # ML prediction fields
    ml_probability: float = 0.0
    ml_signal: str = "NEUTRAL"
    confidence_boost: bool = False

# --- Regime Detection ---
class RegimeDetector:
    def __init__(self, db_path):
        self.db_path = db_path

    def _get_db_connection(self):
        return sqlite3.connect(self.db_path)

    def detect_regime(self, symbol="BTCUSDT", window=20):
        try:
            conn = self._get_db_connection()
            two_days_ago_ms = (datetime.now() - timedelta(days=2)).timestamp() * 1000
            query = f"""
                SELECT close FROM candles
                WHERE symbol = ? AND timestamp >= ?
                ORDER BY timestamp DESC LIMIT ?
            """
            df = pd.read_sql_query(query, conn, params=(symbol, two_days_ago_ms, window + 5))
            conn.close()

            if len(df) < window:
                return {"regime": "unknown", "confidence": 0.0, "details": "Insufficient BTC data for regime detection"}

            df['close'] = pd.to_numeric(df['close'])
            df = df.iloc[::-1].reset_index(drop=True)

            df['sma'] = df['close'].rolling(window=window).mean()
            current_price = df['close'].iloc[-1]
            current_sma = df['sma'].iloc[-1]

            if pd.isna(current_sma):
                 return {"regime": "unknown", "confidence": 0.0, "details": "SMA calculation failed for regime"}

            if current_price > current_sma * 1.01:
                return {"regime": "trending_bull", "confidence": 0.7, "details": f"BTC > {window}-period SMA"}
            elif current_price < current_sma * 0.99:
                return {"regime": "trending_bear", "confidence": 0.7, "details": f"BTC < {window}-period SMA"}
            else:
                return {"regime": "ranging", "confidence": 0.5, "details": f"BTC near {window}-period SMA"}
        except Exception as e:
            logger.error(f"Error in regime detection: {e}", exc_info=True)
            return {"regime": "error", "confidence": 0.0, "details": str(e)}

# --- Enhanced Scoring Engine ---
class EnhancedScoringEngine:
    """Institutional-grade scoring engine with ML capabilities"""
    
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.model = None
        self.scaler = None
        self.is_trained = False
        self.feature_names = [
            'rsi', 'price_1h', 'price_4h', 'price_24h', 'volume_spike_factor',
            'support_distance', 'resistance_distance', 'timeframe_bullish_count',
            'timeframe_bearish_count', 'volatility', 'momentum_3d', 'momentum_7d'
        ]
        self._load_model()
    
    def _load_model(self):
        """Load trained model and scaler if available"""
        try:
            if os.path.exists(ML_MODEL_PATH) and os.path.exists(SCALER_PATH):
                self.model = joblib.load(ML_MODEL_PATH)
                self.scaler = joblib.load(SCALER_PATH)
                self.is_trained = True
                logger.info("✅ Loaded trained ML model and scaler")
            else:
                logger.info("📊 No trained model found - will use rule-based scoring")
        except Exception as e:
            logger.warning(f"⚠️ Error loading ML model: {e}")
            self.is_trained = False
    
    def extract_features(self, df: pd.DataFrame, current_price: float) -> List[float]:
        """Extract feature vector for ML model"""
        features = []
        
        if len(df) < 20:
            return [0.0] * len(self.feature_names)
        
        try:
            # RSI
            rsi = self._calculate_rsi(df['close'])
            features.append(rsi)
            
            # Price changes
            price_1h = ((current_price - df["close"].iloc[-60]) / df["close"].iloc[-60] * 100) if len(df) >= 61 else 0.0
            price_4h = ((current_price - df["close"].iloc[-240]) / df["close"].iloc[-240] * 100) if len(df) >= 241 else 0.0  
            price_24h = ((current_price - df["close"].iloc[-1440]) / df["close"].iloc[-1440] * 100) if len(df) >= 1441 else 0.0
            features.extend([price_1h, price_4h, price_24h])
            
            # Volume spike factor
            rolling_mean_vol = df["volume"].rolling(20).mean()
            current_volume = df["volume"].iloc[-2] if len(df) >= 2 else df["volume"].iloc[-1]
            avg_volume = rolling_mean_vol.iloc[-2] if len(rolling_mean_vol) >= 2 else rolling_mean_vol.iloc[-1]
            volume_spike_factor = current_volume / avg_volume if avg_volume > 0 else 1.0
            features.append(volume_spike_factor)
            
            # Support/Resistance distances
            support_levels = self._detect_support_levels(df)
            resistance_levels = self._detect_resistance_levels(df)
            
            support_distance = min([abs(current_price - s) / current_price for s in support_levels]) if support_levels else 0.1
            resistance_distance = min([abs(current_price - r) / current_price for r in resistance_levels]) if resistance_levels else 0.1
            features.extend([support_distance, resistance_distance])
            
            # Timeframe alignment
            timeframe_bullish = sum([price_1h > 0, price_4h > 0, price_24h > -2])
            timeframe_bearish = sum([price_1h < 0, price_4h < 0, price_24h < 2])
            features.extend([timeframe_bullish, timeframe_bearish])
            
            # Volatility
            volatility = df['close'].pct_change().rolling(20).std().iloc[-1] * 100
            features.append(volatility if not pd.isna(volatility) else 0.0)
            
            # Momentum indicators
            momentum_3d = ((current_price - df["close"].iloc[-4320]) / df["close"].iloc[-4320] * 100) if len(df) >= 4321 else 0.0
            momentum_7d = ((current_price - df["close"].iloc[-10080]) / df["close"].iloc[-10080] * 100) if len(df) >= 10081 else 0.0
            features.extend([momentum_3d, momentum_7d])
            
        except Exception as e:
            logger.warning(f"Error extracting features: {e}")
            features = [0.0] * len(self.feature_names)
        
        return features[:len(self.feature_names)]
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> float:
        """Calculate RSI"""
        if len(prices) < period + 1:
            return 50.0
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi.iloc[-1] if not pd.isna(rsi.iloc[-1]) else 50.0
    
    def _detect_support_levels(self, df: pd.DataFrame) -> List[float]:
        """Detect support levels"""
        if len(df) < 50:
            return []
        
        lows = df['low'].rolling(10).min()
        support_levels = []
        
        for i in range(10, len(lows) - 10):
            if lows.iloc[i] == df['low'].iloc[i]:
                support_levels.append(df['low'].iloc[i])
        
        return list(set(support_levels))[-5:]  # Last 5 support levels
    
    def _detect_resistance_levels(self, df: pd.DataFrame) -> List[float]:
        """Detect resistance levels"""
        if len(df) < 50:
            return []
        
        highs = df['high'].rolling(10).max()
        resistance_levels = []
        
        for i in range(10, len(highs) - 10):
            if highs.iloc[i] == df['high'].iloc[i]:
                resistance_levels.append(df['high'].iloc[i])
        
        return list(set(resistance_levels))[-5:]  # Last 5 resistance levels
    
    def calculate_correlation_penalty(self, symbol: str, returns_vec: List[float]) -> Tuple[float, float, float]:
        """Calculate correlation with BTC/ETH and apply penalty"""
        try:
            if len(returns_vec) < CORRELATION_WINDOW:
                return 0.0, 0.0, 0.0
            
            # Get BTC and ETH returns for correlation
            btc_returns = self._get_correlation_returns('BTCUSDT')
            eth_returns = self._get_correlation_returns('ETHUSDT')
            
            if not btc_returns or not eth_returns:
                return 0.0, 0.0, 0.0
            
            # Calculate correlations
            symbol_returns = pd.Series(returns_vec[-CORRELATION_WINDOW:])
            btc_corr = symbol_returns.corr(pd.Series(btc_returns[-CORRELATION_WINDOW:]))
            eth_corr = symbol_returns.corr(pd.Series(eth_returns[-CORRELATION_WINDOW:]))
            
            # Calculate penalty based on average correlation
            avg_correlation = (abs(btc_corr) + abs(eth_corr)) / 2
            penalty = avg_correlation * CORRELATION_PENALTY_FACTOR
            
            return btc_corr if not pd.isna(btc_corr) else 0.0, eth_corr if not pd.isna(eth_corr) else 0.0, penalty
            
        except Exception as e:
            logger.warning(f"Error calculating correlation for {symbol}: {e}")
            return 0.0, 0.0, 0.0
    
    def _get_correlation_returns(self, symbol: str) -> List[float]:
        """Get returns vector for correlation calculation"""
        try:
            conn = sqlite3.connect(self.db_path)
            query = """
                SELECT close FROM candles 
                WHERE symbol = ? 
                ORDER BY timestamp DESC 
                LIMIT ?
            """
            df = pd.read_sql_query(query, conn, params=(symbol, CORRELATION_WINDOW + 1))
            conn.close()
            
            if len(df) < 2:
                return []
            
            returns = df['close'].pct_change().dropna().tolist()
            return returns[-CORRELATION_WINDOW:] if len(returns) >= CORRELATION_WINDOW else returns
            
        except Exception as e:
            logger.warning(f"Error getting returns for {symbol}: {e}")
            return []
    
    def predict_scores(self, features: List[float]) -> Tuple[float, float, float]:
        """Predict long/short scores using ML model"""
        if not self.is_trained or not ML_AVAILABLE:
            return 0.0, 0.0, 0.0
        
        try:
            # Reshape and scale features
            X = np.array(features).reshape(1, -1)
            X_scaled = self.scaler.transform(X)
            
            # Get prediction probabilities
            proba = self.model.predict_proba(X_scaled)[0]
            
            # Convert to scores (0-100)
            long_score = int(proba[1] * 100) if len(proba) > 1 else 0  # Positive class probability
            short_score = int(proba[0] * 100) if len(proba) > 0 else 0  # Negative class probability
            confidence = max(proba)
            
            return float(long_score), float(short_score), float(confidence)
            
        except Exception as e:
            logger.warning(f"Error in ML prediction: {e}")
            return 0.0, 0.0, 0.0

# --- Main Analyzer Class ---
class EnhancedTokenAnalyzer:
    def __init__(self, db_path: str):
        self.db_path = db_path
        self.regime_detector = RegimeDetector(db_path)
        self.token_list = DEFAULT_TOKENS
        self.scoring_engine = EnhancedScoringEngine(db_path)

    def _get_db_connection(self):
        return sqlite3.connect(self.db_path)

    async def _fetch_historical_data_from_db(self, symbol: str) -> pd.DataFrame:
        """Fetch historical data from database"""
        try:
            query = """
                SELECT open, high, low, close, volume, timestamp
                FROM candles 
                WHERE symbol = ?
                ORDER BY timestamp DESC
                LIMIT ?
            """
            
            with sqlite3.connect(self.db_path) as conn:
                df = pd.read_sql_query(query, conn, params=(symbol, ANALYSIS_CANDLE_LIMIT))
                
            if df.empty:
                logger.warning(f"❌ No historical data found for {symbol}")
                return None
                
            # Convert timestamp and sort chronologically  
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df = df.sort_values('timestamp').reset_index(drop=True)
            
            logger.debug(f"📊 Loaded {len(df)} candles for {symbol}")
            return df
            
        except Exception as e:
            logger.error(f"❌ Database error for {symbol}: {e}")
            return None

    def _get_historical_data(self, symbol: str) -> pd.DataFrame:
        """Legacy method for backwards compatibility"""
        import asyncio
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(self._fetch_historical_data_from_db(symbol))

    async def validate_symbols(self, session: aiohttp.ClientSession) -> List[str]:
        """Validate symbols against Binance API, with proper URL validation"""
        # Guard against invalid URLs to prevent aiohttp.InvalidURL noise
        if not TOKEN_LIST_URL or not TOKEN_LIST_URL.startswith(("http://", "https://")):
            logger.info("Using DEFAULT_TOKENS (30) - no valid TOKEN_LIST_URL configured.")
            self.token_list = DEFAULT_TOKENS
            return DEFAULT_TOKENS
            
        try:
            async with session.get(TOKEN_LIST_URL, timeout=API_TIMEOUT) as response:
                if response.status == 200:
                    data = await response.json()
                    valid_symbols_set = {s['symbol'] for s in data.get('symbols', []) if s['status'] == 'TRADING' and s['symbol'].endswith('USDT')}
                    
                    validated_list = [s for s in DEFAULT_TOKENS if s in valid_symbols_set]
                    if not validated_list and DEFAULT_TOKENS: 
                        logger.warning(f"None of the DEFAULT_TOKENS ({DEFAULT_TOKENS}) are currently valid trading USDT pairs. Check config.")
                    
                    logger.info(f"Validated {len(validated_list)} symbols from DEFAULT_TOKENS for analysis.")
                    self.token_list = validated_list if validated_list else DEFAULT_TOKENS 
                    return self.token_list
                else:
                    logger.warning(f"Failed to fetch exchange info (status {response.status}). Using configured DEFAULT_TOKENS: {DEFAULT_TOKENS}")
        except Exception as e:
            logger.error(f"Error validating symbols: {e}. Using configured DEFAULT_TOKENS: {DEFAULT_TOKENS}")
        
        self.token_list = DEFAULT_TOKENS 
        return DEFAULT_TOKENS

    async def _fetch_current_price(self, symbol: str, session: aiohttp.ClientSession) -> float:
        """Fetch current price for a symbol from Binance API"""
        try:
            url = f"https://api.binance.com/api/v3/ticker/price?symbol={symbol}"
            timeout = aiohttp.ClientTimeout(total=10)
            async with session.get(url, timeout=timeout) as response:
                if response.status == 200:
                    data = await response.json()
                    return float(data['price'])
                else:
                    logger.error(f"❌ API error for {symbol}: {response.status}")
                    return 0.0
        except Exception as e:
            logger.error(f"❌ Error fetching current price for {symbol}: {e}")
            return 0.0

    def _calculate_rsi(self, prices: pd.Series, period: int = RSI_PERIOD) -> float:
        if prices is None or len(prices) < period + 1: 
            return 50.0
        try:
            delta = prices.diff()
            gain = (delta.where(delta > 0, 0.0)).rolling(window=period).mean()
            loss = (-delta.where(delta < 0, 0.0)).rolling(window=period).mean()
            
            if loss.iloc[-1] == 0: 
                return 100.0 if gain.iloc[-1] > 0 else 50.0

            rs = gain.iloc[-1] / loss.iloc[-1]
            rsi = 100.0 - (100.0 / (1.0 + rs))
            return rsi if not pd.isna(rsi) else 50.0
        except Exception as e:
            logger.error(f"Error calculating RSI: {e}", exc_info=True)
            return 50.0


    def detect_support_levels(self, df: pd.DataFrame) -> List[float]:
        try:
            if len(df) < 40: return []
            lows = df["low"].values
            support_levels = []
            for i in range(5, len(lows) - 5):
                if lows[i] == min(lows[i-5:i+6]):
                    support_levels.append(float(lows[i]))
            return sorted(list(set(support_levels)), reverse=False)[:3]
        except Exception as e:
            logger.error(f"Error detecting support levels: {e}", exc_info=True)
            return []

    def detect_resistance_levels(self, df: pd.DataFrame) -> List[float]:
        try:
            if len(df) < 40: return []
            highs = df["high"].values
            resistance_levels = []
            for i in range(5, len(highs) - 5):
                if highs[i] == max(highs[i-5:i+6]):
                    resistance_levels.append(float(highs[i]))
            return sorted(list(set(resistance_levels)), reverse=True)[:3]
        except Exception as e:
            logger.error(f"Error detecting resistance levels: {e}", exc_info=True)
            return []

    def calculate_volatility(self, prices: pd.Series) -> float:
        try:
            if prices is None or len(prices) < 21: return 0.0
            returns = prices.pct_change().dropna()
            if len(returns) < 20: return 0.0
            volatility_val = returns.rolling(window=20).std().iloc[-1]
            annualized_volatility = float(volatility_val * np.sqrt(365 * 24 * 60) * 100)
            return annualized_volatility if not np.isnan(annualized_volatility) else 0.0
        except Exception as e:
            logger.error(f"Error calculating volatility: {e}", exc_info=True)
            return 0.0

    def calculate_returns_vector(self, prices: pd.Series) -> List[float]:
        try:
            if prices is None or len(prices) < 11: return []
            returns = prices.pct_change().dropna()
            if len(returns) < 10: return []
            recent_returns = returns.tail(10).tolist()
            return [float(r) for r in recent_returns if not np.isnan(r)]
        except Exception as e:
            logger.error(f"Error calculating returns vector: {e}", exc_info=True)
            return []

    def _create_empty_opportunity(self, symbol: str) -> TokenOpportunity:
        return TokenOpportunity(
            symbol=symbol,
            entry_recommendation="INSUFFICIENT_DATA"
        )

    async def analyze_token_async(self, symbol: str, session: aiohttp.ClientSession) -> Optional[TokenOpportunity]:
        """Enhanced analysis with institutional-grade scoring"""
        try:
            # Fetch current price
            current_price = await self._fetch_current_price(symbol, session)
            if current_price is None:
                logger.warning(f"❌ Could not fetch current price for {symbol}")
                return None

            # Get historical data
            df = await self._fetch_historical_data_from_db(symbol)
            if df is None or len(df) < 50:
                logger.warning(f"❌ Insufficient data for {symbol}")
                return None

            # Calculate price changes
            price_1h = ((current_price - df["close"].iloc[-60]) / df["close"].iloc[-60] * 100) if len(df) >= 61 else 0.0
            price_4h = ((current_price - df["close"].iloc[-240]) / df["close"].iloc[-240] * 100) if len(df) >= 241 else 0.0  
            price_24h = ((current_price - df["close"].iloc[-1440]) / df["close"].iloc[-1440] * 100) if len(df) >= 1441 else 0.0

            # Enhanced feature extraction
            features = self.scoring_engine.extract_features(df, current_price)
            
            # Calculate RSI
            rsi_val = self._calculate_rsi(df['close'])
            
            # Calculate returns for correlation
            returns_vec = df['close'].pct_change().dropna().tolist()[-CORRELATION_WINDOW:] if len(df) > CORRELATION_WINDOW else []
            
            # Volume analysis
            rolling_mean_vol = df["volume"].rolling(20).mean()
            current_candle_volume = df["volume"].iloc[-2] if len(df) >= 2 else df["volume"].iloc[-1]
            avg_volume_20 = rolling_mean_vol.iloc[-2] if len(rolling_mean_vol) >= 2 else rolling_mean_vol.iloc[-1]
            volume_spike = avg_volume_20 > 0 and current_candle_volume > (avg_volume_20 * VOLUME_SPIKE_FACTOR)

            # Technical analysis conditions
            support_bounce_condition = self._check_support_bounce(df, current_price)
            resistance_rejection_condition = self._check_resistance_rejection(df, current_price)
            multi_timeframe_alignment = self._check_multi_timeframe_alignment(df, current_price)
            failed_breakout_condition = self._check_failed_breakout(df, current_price)

            # Count timeframe alignments
            bullish_timeframes = sum([price_1h > 0, price_4h > 0, price_24h > -2])
            bearish_timeframes = sum([price_1h < 0, price_4h < 0, price_24h < 2])

            # Initialize scoring variables
            raw_long_score, raw_short_score = 0.0, 0.0
            ml_confidence = 0.0
            scoring_method = "rule_based"

            # Data-driven scoring vs rule-based
            if DATA_DRIVEN_SCORING and self.scoring_engine.is_trained:
                # Use ML model
                ml_long, ml_short, ml_confidence = self.scoring_engine.predict_scores(features)
                raw_long_score, raw_short_score = ml_long, ml_short
                scoring_method = "ml_model"
                logger.debug(f"🤖 ML scores for {symbol}: Long={ml_long:.1f}, Short={ml_short:.1f}, Conf={ml_confidence:.2f}")
            else:
                # Enhanced rule-based scoring (original logic improved)
                # Volume + RSI combinations
                if avg_volume_20 > 0 and current_candle_volume > (avg_volume_20 * VOLUME_SPIKE_FACTOR):
                    if rsi_val < RSI_OVERSOLD + 5: 
                        raw_long_score += 40
                    if rsi_val > RSI_OVERBOUGHT - 5: 
                        raw_short_score += 40

                # Technical patterns
                if support_bounce_condition:
                    raw_long_score += 30
                if resistance_rejection_condition:
                    raw_short_score += 30

                # Multi-timeframe alignment
                if bullish_timeframes >= 2:
                    raw_long_score += 25
                if bearish_timeframes >= 2:
                    raw_short_score += 25

                # Failed breakouts
                if failed_breakout_condition:
                    raw_short_score += 20

                # Additional volume patterns
                if volume_spike and rsi_val < 30:
                    raw_long_score += 15  # Volume spike at oversold
                if volume_spike and rsi_val > 70:
                    raw_short_score += 15  # Volume spike at overbought

                scoring_method = "rule_based_enhanced"

            # Correlation penalty
            btc_corr, eth_corr, correlation_penalty = self.scoring_engine.calculate_correlation_penalty(symbol, returns_vec)
            
            # Market regime adjustment
            regime_info = self.regime_detector.detect_regime()
            regime_multiplier = 1.0
            
            if regime_info['regime'] == 'BEAR' and regime_info['confidence'] > 0.7:
                regime_multiplier = REGIME_MULTIPLIER_BEAR
                raw_long_score *= regime_multiplier  # Reduce long bias in bear market
            elif regime_info['regime'] == 'BULL' and regime_info['confidence'] > 0.7:
                regime_multiplier = REGIME_MULTIPLIER_BULL
                raw_short_score *= regime_multiplier  # Reduce short bias in bull market

            # Apply correlation penalty
            final_long_score = max(0, raw_long_score - (raw_long_score * correlation_penalty))
            final_short_score = max(0, raw_short_score - (raw_short_score * correlation_penalty))

            # Binary decisions
            simple_long_action_val = "BUY_LONG" if final_long_score >= BUY_THRESHOLD else "DON'T_BUY_LONG"
            simple_short_action_val = "SHORT" if final_short_score >= BUY_THRESHOLD else "DON'T_SHORT"

            # Determine primary direction
            if final_long_score > final_short_score and final_long_score >= BUY_THRESHOLD:
                direction = "LONG"
                entry_recommendation = "BUY_LONG"
            elif final_short_score > final_long_score and final_short_score >= BUY_THRESHOLD:
                direction = "SHORT"
                entry_recommendation = "SHORT"
            else:
                direction = "NEUTRAL"
                entry_recommendation = "WAIT"

            # Volatility calculation
            volatility = df['close'].pct_change().rolling(20).std().iloc[-1] * 100 if len(df) >= 20 else 0.0
            
            logger.info(f"📊 {symbol}: {scoring_method.upper()} | Raw: L={raw_long_score:.1f}/S={raw_short_score:.1f} | "
                       f"Final: L={final_long_score:.1f}/S={final_short_score:.1f} | Corr={correlation_penalty:.2f} | "
                       f"Action: {simple_long_action_val}/{simple_short_action_val}")

            return TokenOpportunity(
                symbol=symbol,
                current_price=current_price,
                price_change_1h=price_1h,
                price_change_4h=price_4h,
                price_change_24h=price_24h,
                rsi=rsi_val,
                volume_spike=volume_spike,
                support_bounce=support_bounce_condition,
                resistance_rejection=resistance_rejection_condition,
                multi_timeframe_alignment=multi_timeframe_alignment,
                composite_score=max(final_long_score, final_short_score),
                entry_recommendation=entry_recommendation,
                volatility=volatility if not pd.isna(volatility) else 0.0,
                returns_vec=returns_vec,
                # Enhanced scoring fields
                long_score=final_long_score,
                short_score=final_short_score,
                raw_long_score=raw_long_score,
                raw_short_score=raw_short_score,
                correlation_btc=btc_corr,
                correlation_eth=eth_corr,
                correlation_penalty=correlation_penalty,
                regime_multiplier=regime_multiplier,
                direction=direction,
                simple_long_action=simple_long_action_val,
                simple_short_action=simple_short_action_val,
                feature_vector=features,
                ml_confidence=ml_confidence,
                scoring_method=scoring_method
            )

        except Exception as e:
            logger.error(f"❌ Error analyzing {symbol}: {e}", exc_info=True)
            return None

    def add_percentile_ranking(self, opportunities: List[TokenOpportunity]) -> List[TokenOpportunity]:
        """Add cross-sectional percentile ranking to adapt to regime drift"""
        if not opportunities or not PERCENTILE_RANKING:
            return opportunities
        
        try:
            # Extract scores for ranking
            long_scores = [opp.long_score for opp in opportunities if opp.long_score is not None]
            short_scores = [opp.short_score for opp in opportunities if opp.short_score is not None]
            
            if not long_scores or not short_scores:
                logger.warning("⚠️ No valid scores for percentile ranking")
                return opportunities
            
            # Calculate percentiles
            for opp in opportunities:
                if opp.long_score is not None:
                    opp.percentile_long = sum(1 for score in long_scores if score <= opp.long_score) / len(long_scores) * 100
                if opp.short_score is not None:
                    opp.percentile_short = sum(1 for score in short_scores if score <= opp.short_score) / len(short_scores) * 100
            
            # Log percentile distribution
            long_percentiles = [opp.percentile_long for opp in opportunities if opp.percentile_long is not None]
            short_percentiles = [opp.percentile_short for opp in opportunities if opp.percentile_short is not None]
            
            logger.info(f"📊 Percentile Distribution - Long: {np.mean(long_percentiles):.1f}±{np.std(long_percentiles):.1f}, "
                       f"Short: {np.mean(short_percentiles):.1f}±{np.std(short_percentiles):.1f}")
            
            return opportunities
            
        except Exception as e:
            logger.error(f"❌ Error in percentile ranking: {e}")
            return opportunities

    def save_analysis_results(self, opportunities: List[TokenOpportunity], market_regime_info: Optional[Dict] = None, perf_metrics: Optional[Dict] = None):
        results_metadata = {
            "tokens_analyzed": len(opportunities),
            "analysis_version": "enhanced_v2.4_cascade_final", 
        }
        if market_regime_info:
            results_metadata.update({
                "market_regime": market_regime_info.get("regime", "UNKNOWN"),
                "market_regime_confidence": market_regime_info.get("confidence", 0.0),
                "market_regime_details": market_regime_info.get("details", "")
            })
        if perf_metrics:
             results_metadata["performance_metrics"] = perf_metrics

        output_data = {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "metadata": results_metadata,
            "opportunities": []
        }
        for opp in opportunities:
            output_data["opportunities"].append({
                "symbol": opp.symbol, "current_price": opp.current_price,
                "composite_score": opp.composite_score, "long_score": opp.long_score, "short_score": opp.short_score,
                "direction": opp.direction, "entry_recommendation": opp.entry_recommendation,
                "price_changes": {"1h": opp.price_change_1h, "4h": opp.price_change_4h, "24h": opp.price_change_24h},
                "technical_indicators": {
                    "rsi": opp.rsi, "volume_spike": opp.volume_spike, "support_bounce": opp.support_bounce,
                    "resistance_rejection": opp.resistance_rejection, "multi_timeframe_alignment": opp.multi_timeframe_alignment
                },
                "simple_long_action": opp.simple_long_action, "simple_short_action": opp.simple_short_action,
                "volatility": opp.volatility,
                # ML prediction fields
                "ml_probability": getattr(opp, 'ml_probability', None),
                "ml_signal": getattr(opp, 'ml_signal', None), 
                "ml_confidence": getattr(opp, 'ml_confidence', None),
                "confidence_boost": getattr(opp, 'confidence_boost', 0.0)
            })
        try:
            with open(OUTPUT_FILE, 'w') as f: json.dump(output_data, f, indent=2, cls=CustomJSONEncoder)
            logger.info(f"Analysis results saved to {OUTPUT_FILE}")
            with open(DEBUG_FILE, 'w') as f: json.dump(output_data, f, indent=2, cls=CustomJSONEncoder) 
            logger.info(f"Debug analysis results saved to {DEBUG_FILE}")
        except Exception as e:
            logger.error(f"Error saving analysis results: {e}", exc_info=True)

    def _check_support_bounce(self, df: pd.DataFrame, current_price: float) -> bool:
        """Check if current price is bouncing from support level"""
        try:
            if len(df) < 50:
                return False
            
            support_levels = self.detect_support_levels(df)
            if not support_levels:
                return False
            
            nearest_support = min(support_levels, key=lambda x: abs(x - current_price))
            distance_pct = abs(current_price - nearest_support) / current_price
            
            return distance_pct < 0.01 and current_price > nearest_support
        except:
            return False

    def _check_resistance_rejection(self, df: pd.DataFrame, current_price: float) -> bool:
        """Check if current price is rejecting from resistance level"""
        try:
            if len(df) < 50:
                return False
            
            resistance_levels = self.detect_resistance_levels(df)
            if not resistance_levels:
                return False
            
            nearest_resistance = min(resistance_levels, key=lambda x: abs(x - current_price))
            distance_pct = abs(current_price - nearest_resistance) / current_price
            
            return distance_pct < 0.01 and current_price < nearest_resistance
        except:
            return False

    def _check_multi_timeframe_alignment(self, df: pd.DataFrame, current_price: float) -> bool:
        """Check if multiple timeframes are aligned"""
        try:
            if len(df) < 1440:
                return False
            
            price_1h = ((current_price - df["close"].iloc[-60]) / df["close"].iloc[-60] * 100) if len(df) >= 61 else 0.0
            price_4h = ((current_price - df["close"].iloc[-240]) / df["close"].iloc[-240] * 100) if len(df) >= 241 else 0.0  
            price_24h = ((current_price - df["close"].iloc[-1440]) / df["close"].iloc[-1440] * 100) if len(df) >= 1441 else 0.0
            
            bullish_count = sum([price_1h > 0, price_4h > 0, price_24h > -2])
            bearish_count = sum([price_1h < 0, price_4h < 0, price_24h < 2])
            
            return bullish_count >= 2 or bearish_count >= 2
        except:
            return False

    def _check_failed_breakout(self, df: pd.DataFrame, current_price: float) -> bool:
        """Check if there's a failed breakout pattern"""
        try:
            if len(df) < 50:
                return False
            
            rolling_high_window = min(20, len(df) - 1)
            high_20 = df["high"].rolling(rolling_high_window).max().iloc[-rolling_high_window]
            
            recent_high = df["high"].tail(5).max()
            
            return recent_high > high_20 * 0.995 and current_price < high_20
        except:
            return False

async def main():
    start_time_main = time.time()
    logger.info(f"Starting enhanced multi-token analysis (version: enhanced_v2.4_cascade_final) from {__file__}...")
    
    analyzer = EnhancedTokenAnalyzer(DB_PATH)
    market_regime_info = analyzer.regime_detector.detect_regime()
    logger.info(f"Market Regime Detected: {market_regime_info}")
    
    async with aiohttp.ClientSession() as session:
        api_fetch_start = time.time()
        symbols_to_analyze = await analyzer.validate_symbols(session)
        api_fetch_duration = (time.time() - api_fetch_start) * 1000
        logger.info(f"Symbols validated in {api_fetch_duration:.0f}ms. Analyzing {len(symbols_to_analyze)} symbols: {str(symbols_to_analyze)[:200]}")


        if not symbols_to_analyze:
            logger.warning("No symbols to analyze after validation. Saving empty result.")
            perf = {"api_fetch_ms": int(api_fetch_duration), "analysis_compute_ms": 0, "total_elapsed_ms": int((time.time() - start_time_main) * 1000), "tokens_requested": 0, "tokens_successfully_processed": 0}
            analyzer.save_analysis_results([], market_regime_info, perf_metrics=perf)
            return

        analysis_compute_start = time.time()
        tasks = [analyzer.analyze_token_async(symbol, session) for symbol in symbols_to_analyze]
        analysis_results = await asyncio.gather(*tasks, return_exceptions=True)
        analysis_compute_duration = (time.time() - analysis_compute_start) * 1000
        
        # Filter valid results and handle exceptions
        valid_opportunities = []
        for i, result in enumerate(analysis_results):
            if isinstance(result, Exception):
                logger.error(f"❌ Analysis failed for {symbols_to_analyze[i]}: {result}")
            elif result is not None:
                valid_opportunities.append(result)
        
        # Apply cross-sectional percentile ranking 
        if valid_opportunities:
            valid_opportunities = analyzer.add_percentile_ranking(valid_opportunities)
        
        # Integrate optimized ML predictions
        if valid_opportunities:
            valid_opportunities = integrate_optimized_ml_predictions(valid_opportunities)
        
        # Detect market regime
        regime_detect_start = time.time()
        market_regime_info = analyzer.regime_detector.detect_regime()
        regime_detect_duration = (time.time() - regime_detect_start) * 1000
        
        valid_opportunities.sort(key=lambda x: x.composite_score, reverse=True)
    
    total_elapsed_main = (time.time() - start_time_main) * 1000
    
    successfully_processed_count = len([
        opp for opp in valid_opportunities 
        if opp.entry_recommendation != "INSUFFICIENT_DATA"
    ])

    performance_metrics = {
        "api_fetch_ms": int(api_fetch_duration),
        "analysis_compute_ms": int(analysis_compute_duration),
        "total_elapsed_ms": int(total_elapsed_main),
        "tokens_requested": len(symbols_to_analyze),
        "tokens_successfully_processed": successfully_processed_count
    }
    
    analyzer.save_analysis_results(valid_opportunities, market_regime_info, perf_metrics=performance_metrics)
    
    logger.info(f"Enhanced analysis complete: {len(valid_opportunities)} tokens results generated in {total_elapsed_main:.0f}ms ({successfully_processed_count} successfully processed). Output: {OUTPUT_FILE}")
    if valid_opportunities and successfully_processed_count > 0 :
        top_real_opp = next((opp for opp in valid_opportunities if opp.entry_recommendation != "INSUFFICIENT_DATA"), None)
        if top_real_opp:
             logger.info(f"Top scorable opportunity: {top_real_opp.symbol} (L: {top_real_opp.long_score}, S: {top_real_opp.short_score}, Rec: {top_real_opp.entry_recommendation})")
        else:
            logger.info("No scorable opportunities found despite processing.")
    else:
        logger.info("No scorable opportunities found or processed.")

def integrate_optimized_ml_predictions(opportunities: List[TokenOpportunity]) -> List[TokenOpportunity]:
    """Integrate optimized ML predictions using the new LightGBM pipeline"""
    try:
        from ml_inference_engine import MLInferenceEngine
        
        logger.info("🧠 Integrating optimized ML predictions...")
        ml_engine = MLInferenceEngine(models_dir="models")
        
        if not ml_engine.load_latest_model():
            logger.warning("⚠️ Optimized ML model not available, using existing predictions")
            return opportunities
        
        # Get symbols to predict
        symbols = [opp.symbol for opp in opportunities]
        ml_predictions = ml_engine.batch_predict(symbols)
        
        # Enhance opportunities with ML predictions
        for opp in opportunities:
            if opp.symbol in ml_predictions:
                ml_pred = ml_predictions[opp.symbol]['ml_prediction']
                
                # Update scores with ML predictions
                opp.ml_probability = ml_pred['probability']
                opp.ml_confidence = ml_pred['confidence']
                opp.ml_signal = ml_pred['signal']
                
                # If ML confidence is high, boost the score
                if ml_pred['confidence'] > 0.7:
                    if ml_pred['signal'] == 'BUY_LONG' and opp.simple_long_action == 'BUY_LONG':
                        opp.long_score = min(100, opp.long_score * 1.15)  # 15% boost for high-confidence ML agreement
                        opp.confidence_boost = True
                    elif ml_pred['signal'] == 'SHORT' and opp.simple_short_action == 'SHORT':
                        opp.short_score = min(100, opp.short_score * 1.15)  # 15% boost for high-confidence ML agreement
                        opp.confidence_boost = True
                    elif ml_pred['signal'] == 'NEUTRAL':
                        opp.long_score = max(0, opp.long_score * 0.9)  # Reduce score if ML disagrees
                        opp.short_score = max(0, opp.short_score * 0.9)  # Reduce score if ML disagrees
                        opp.confidence_boost = False
        
        enhanced_count = len([opp for opp in opportunities if hasattr(opp, 'ml_probability')])
        logger.info(f"✅ Enhanced {enhanced_count}/{len(opportunities)} opportunities with optimized ML")
        
        return opportunities
        
    except ImportError:
        logger.warning("⚠️ ML inference engine not available")
        return opportunities
    except Exception as e:
        logger.error(f"❌ ML integration failed: {e}")
        return opportunities

if __name__ == "__main__":
    asyncio.run(main())
