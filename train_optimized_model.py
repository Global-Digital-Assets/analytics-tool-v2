#!/usr/bin/env python3
"""
ðŸš€ OPTIMIZED ML TRAINING PIPELINE
- Polars for 10x faster feature engineering
- LightGBM for 4x faster training + better accuracy
- 5-minute downsampling for 5x data reduction
- Target: <5 minutes total training time
"""

import logging
import sqlite3
import time
from datetime import datetime, timedelta
from typing import Dict, List, Tuple

import polars as pl
import lightgbm as lgb
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, roc_auc_score
import joblib

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('optimized_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class OptimizedMLTrainer:
    """High-performance ML trainer using Polars + LightGBM"""
    
    def __init__(self, db_path: str = "market_data.db"):
        self.db_path = db_path
        self.symbols = [
            'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'SOLUSDT', 'XRPUSDT',
            'ADAUSDT', 'DOTUSDT', 'AVAXUSDT', 'LINKUSDT', 'ATOMUSDT',
            'MATICUSDT', 'ALGOUSDT', 'LTCUSDT', 'BCHUSDT', 'FILUSDT',
            'TRXUSDT', 'VETUSDT', 'XLMUSDT', 'ICPUSDT', 'THETAUSDT',
            'EOSUSDT', 'AXSUSDT', 'SANDUSDT', 'MANAUSDT', 'GALAUSDT',
            'CHZUSDT', 'ENJUSDT', 'ZILUSDT', 'BATUSDT', 'ZECUSDT'
        ]
        
    def load_raw_data(self) -> pl.DataFrame:
        """Load raw 1-minute candle data using Polars for maximum speed"""
        logger.info("ðŸ“Š Loading raw candle data with Polars...")
        
        conn = sqlite3.connect(self.db_path)
        
        # Use Polars to read directly from SQLite - much faster than pandas
        query = """
        SELECT symbol, timestamp, open, high, low, close, volume
        FROM candles 
        WHERE symbol IN ({})
        ORDER BY symbol, timestamp
        """.format(','.join([f"'{s}'" for s in self.symbols]))
        
        df = pl.read_database(query, conn)
        conn.close()
        
        logger.info(f"âœ… Loaded {len(df):,} raw candle samples")
        return df
    
    def create_5min_bars(self, df: pl.DataFrame) -> pl.DataFrame:
        """Create 5-minute OHLCV bars using Polars (5x speedup)"""
        logger.info("â° Creating 5-minute bars with Polars aggregation...")
        
        df_5m = (
            df
            .with_columns([
                # Create 5-minute timestamp bins
                (pl.col("timestamp") // 300 * 300).alias("ts5m")
            ])
            .group_by(["symbol", "ts5m"])
            .agg([
                pl.col("open").first().alias("open"),
                pl.col("high").max().alias("high"), 
                pl.col("low").min().alias("low"),
                pl.col("close").last().alias("close"),
                pl.col("volume").sum().alias("volume"),
                pl.col("timestamp").count().alias("bars_count")
            ])
            .sort(["symbol", "ts5m"])
        )
        
        logger.info(f"âœ… Created {len(df_5m):,} 5-minute bars ({len(df)/len(df_5m):.1f}x compression)")
        return df_5m
    
    def engineer_features(self, df: pl.DataFrame) -> pl.DataFrame:
        """Engineer ML features using Polars vectorized operations (10x faster)"""
        logger.info("ðŸ”§ Engineering features with Polars vectorization...")
        
        df_features = (
            df
            .sort(["symbol", "ts5m"])
            .with_columns([
                # Price features - calculate returns first
                (pl.col("close") / pl.col("close").shift(1).over("symbol") - 1).alias("ret_5m"),
            ])
            .with_columns([
                # Now use ret_5m for other calculations
                (pl.col("close") / pl.col("close").shift(12).over("symbol") - 1).alias("ret_1h"), 
                (pl.col("close") / pl.col("close").shift(48).over("symbol") - 1).alias("ret_4h"),
                (pl.col("close") / pl.col("close").shift(288).over("symbol") - 1).alias("ret_24h"),
                
                # Volume features
                pl.col("volume").rolling_mean(12).over("symbol").alias("vol_ma_1h"),
                (pl.col("volume") / pl.col("volume").rolling_mean(12).over("symbol")).alias("vol_spike"),
                
                # Volatility features
                pl.col("ret_5m").rolling_std(12).over("symbol").alias("volatility_1h"),
                pl.col("ret_5m").rolling_std(48).over("symbol").alias("volatility_4h"),
                
                # Price momentum
                ((pl.col("high") + pl.col("low")) / 2 / pl.col("close") - 1).alias("hl_spread"),
                (pl.col("close") / pl.col("open") - 1).alias("bar_return"),
                
                # Technical indicators (simplified for speed)
                pl.col("close").rolling_mean(14).over("symbol").alias("sma_14"),
                (pl.col("close") / pl.col("close").rolling_mean(14).over("symbol") - 1).alias("sma_distance"),
                
                # Simple RSI approximation using rolling mean of gains/losses
                pl.when(pl.col("ret_5m") > 0)
                  .then(pl.col("ret_5m"))
                  .otherwise(0)
                  .rolling_mean(14).over("symbol").alias("avg_gains"),
                pl.when(pl.col("ret_5m") < 0)
                  .then(-pl.col("ret_5m"))
                  .otherwise(0)
                  .rolling_mean(14).over("symbol").alias("avg_losses")
            ])
            .with_columns([
                # Calculate RSI from gains/losses
                (100 - 100 / (1 + pl.col("avg_gains") / (pl.col("avg_losses") + 1e-10))).alias("rsi14")
            ])
            .drop(["avg_gains", "avg_losses"])  # Clean up intermediate columns
        )
        
        # Create target variable (predict next hour return > 0.5%)
        df_features = df_features.with_columns([
            (pl.col("ret_1h").shift(-12).over("symbol") > 0.005).alias("target")
        ])
        
        logger.info(f"âœ… Engineered {df_features.width} features for {len(df_features):,} samples")
        return df_features
    
    def prepare_training_data(self) -> Tuple[np.ndarray, np.ndarray]:
        """Prepare final training dataset"""
        logger.info("ðŸ§  Preparing training dataset...")
        
        # Load and process data
        raw_data = self.load_raw_data()
        bars_5m = self.create_5min_bars(raw_data)
        features_df = self.engineer_features(bars_5m)
        
        # Remove NaN values and convert to numpy
        clean_df = features_df.drop_nulls()
        
        feature_cols = [
            'ret_5m', 'ret_1h', 'ret_4h', 'ret_24h',
            'vol_spike', 'volatility_1h', 'volatility_4h', 
            'hl_spread', 'bar_return', 'sma_distance', 'rsi14'
        ]
        
        X = clean_df.select(feature_cols).to_numpy()
        y = clean_df.select("target").to_numpy().flatten()
        
        logger.info(f"âœ… Training data ready: {X.shape[0]:,} samples Ã— {X.shape[1]} features")
        logger.info(f"ðŸ“Š Target distribution: {y.mean():.3f} positive rate")
        
        return X, y
    
    def train_lightgbm_model(self, X: np.ndarray, y: np.ndarray) -> lgb.Booster:
        """Train LightGBM model with 4-core optimization"""
        logger.info("ðŸš€ Training LightGBM model with 4-core parallelism...")
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, shuffle=False, random_state=42
        )
        
        # Create LightGBM datasets
        train_data = lgb.Dataset(X_train, label=y_train)
        test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)
        
        # Optimized LightGBM parameters for 4-core VPS
        params = {
            'objective': 'binary',
            'metric': 'auc',
            'boosting_type': 'gbdt',
            'num_leaves': 63,
            'learning_rate': 0.05,
            'feature_fraction': 0.8,
            'bagging_fraction': 0.8,
            'bagging_freq': 5,
            'min_data_in_leaf': 100,
            'num_threads': 4,  # Use all 4 cores
            'verbosity': 1,
            'random_state': 42
        }
        
        # Train model with early stopping
        start_time = time.time()
        model = lgb.train(
            params,
            train_data,
            num_boost_round=500,
            valid_sets=[test_data],
            callbacks=[
                lgb.early_stopping(50),
                lgb.log_evaluation(100)
            ]
        )
        train_time = time.time() - start_time
        
        # Evaluate model
        y_pred = model.predict(X_test)
        y_pred_binary = (y_pred > 0.5).astype(int)
        
        accuracy = accuracy_score(y_test, y_pred_binary)
        auc = roc_auc_score(y_test, y_pred)
        
        logger.info(f"ðŸŽ¯ Model trained in {train_time:.1f} seconds")
        logger.info(f"ðŸ“Š Test Accuracy: {accuracy:.3f}")
        logger.info(f"ðŸ“ˆ Test AUC: {auc:.3f}")
        
        if accuracy > 0.80:
            logger.info("ðŸŽ‰ SUCCESS: Model exceeds 80% accuracy target!")
        
        return model
    
    def save_model(self, model: lgb.Booster, filename: str = "optimized_crypto_model.joblib"):
        """Save trained model"""
        joblib.dump(model, filename)
        logger.info(f"ðŸ’¾ Model saved as {filename}")
    
    def run_training(self):
        """Run complete optimized training pipeline"""
        start_time = time.time()
        logger.info("ðŸš€ Starting OPTIMIZED ML training pipeline")
        logger.info("âš¡ Polars + LightGBM + 5min bars for maximum speed")
        
        try:
            # Prepare data
            X, y = self.prepare_training_data()
            
            # Train model
            model = self.train_lightgbm_model(X, y)
            
            # Save model
            self.save_model(model)
            
            total_time = time.time() - start_time
            logger.info(f"âœ… TRAINING COMPLETE in {total_time:.1f} seconds ({total_time/60:.1f} minutes)")
            
            if total_time < 300:  # 5 minutes
                logger.info("ðŸŽ¯ SUCCESS: Training completed under 5-minute target!")
            
        except Exception as e:
            logger.error(f"âŒ Training failed: {e}")
            raise

if __name__ == "__main__":
    trainer = OptimizedMLTrainer()
    trainer.run_training()
